{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rxMhvIqeoTw"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwrxX75keoTz",
        "outputId": "c7b2f21f-5cad-4d19-ce8b-8466cf8376a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/jacklynjoaquin/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/jacklynjoaquin/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os #library in managing directories\n",
        "import re, string #library in removing special characters\n",
        "\n",
        "#for text pre-processing\n",
        "import nltk #natural language tool kit\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#for model-building\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "#for feature extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TSajVw1eoT0"
      },
      "source": [
        "## Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVYUD-MVeoT0",
        "outputId": "3ab6abb6-cbb8-4d15-bdc0-3fb9bac4d9a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      id                                               text label  rating\n",
            "0   4715  For a movie that gets no respect there sure ar...   pos       9\n",
            "1  12390  Bizarre horror movie filled with famous faces ...   pos       8\n",
            "2   8329  A solid, if unremarkable film. Matthau, as Ein...   pos       7\n",
            "3   9063  It's a strange feeling to sit alone in a theat...   pos       8\n",
            "4   3092  You probably all already know this by now, but...   pos      10\n",
            "          id                                               text label  rating\n",
            "49995  11513  With actors like Depardieu and Richard it is r...   neg       1\n",
            "49996   5409  If you like to get a couple of fleeting glimps...   neg       1\n",
            "49997  11187  When something can be anything you want it to ...   neg       1\n",
            "49998   9359  I had heard good things about \"States of Grace...   neg       3\n",
            "49999  11556  Well, this movie actually did have one redeemi...   neg       1\n"
          ]
        }
      ],
      "source": [
        "imdb_data = pd.read_csv('/Users/jacklynjoaquin/Documents/IMDB-project/imdb_data_extended.csv')\n",
        "print(imdb_data.head())\n",
        "print(imdb_data.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qZwlZJceoT1",
        "outputId": "d022c4e7-a78c-4a62-81f6-975811b3f87e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id         int64\n",
            "text      object\n",
            "label     object\n",
            "rating     int64\n",
            "dtype: object\n",
            "Data shape:  (50000, 4)\n"
          ]
        }
      ],
      "source": [
        "print(imdb_data.dtypes)\n",
        "print(\"Data shape: \", imdb_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "odTN3wUleoT1",
        "outputId": "9020a36a-e132-48d3-d2ac-422c428acd32"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I'm a fan of Crash and Blade Runner and this movie explores some of those highway death and 80s film noir themes that I like to see, so I enjoyed it.<br /><br />In general though, the essential stupidity of the film noir protagonist is not pulled off well by the female lead and her hero is nearly a neanderthal, hence the kitch warning.\""
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "imdb_data['text'][4000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry_HUrB5eoT1"
      },
      "source": [
        "## Text Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b778Mh_MeoT2",
        "outputId": "bcaa5860-b517-4eaa-d099-ee20fd29d311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          id                                               text label  rating   \n",
            "0       4715  For a movie that gets no respect there sure ar...   pos       9  \\\n",
            "1      12390  Bizarre horror movie filled with famous faces ...   pos       8   \n",
            "2       8329  A solid, if unremarkable film. Matthau, as Ein...   pos       7   \n",
            "3       9063  It's a strange feeling to sit alone in a theat...   pos       8   \n",
            "4       3092  You probably all already know this by now, but...   pos      10   \n",
            "...      ...                                                ...   ...     ...   \n",
            "49995  11513  With actors like Depardieu and Richard it is r...   neg       1   \n",
            "49996   5409  If you like to get a couple of fleeting glimps...   neg       1   \n",
            "49997  11187  When something can be anything you want it to ...   neg       1   \n",
            "49998   9359  I had heard good things about \"States of Grace...   neg       3   \n",
            "49999  11556  Well, this movie actually did have one redeemi...   neg       1   \n",
            "\n",
            "                                               processed  \n",
            "0      [For, a, movie, that, gets, no, respect, there...  \n",
            "1      [Bizarre, horror, movie, filled, with, famous,...  \n",
            "2      [A, solid, ,, if, unremarkable, film, ., Matth...  \n",
            "3      [It, 's, a, strange, feeling, to, sit, alone, ...  \n",
            "4      [You, probably, all, already, know, this, by, ...  \n",
            "...                                                  ...  \n",
            "49995  [With, actors, like, Depardieu, and, Richard, ...  \n",
            "49996  [If, you, like, to, get, a, couple, of, fleeti...  \n",
            "49997  [When, something, can, be, anything, you, want...  \n",
            "49998  [I, had, heard, good, things, about, ``, State...  \n",
            "49999  [Well, ,, this, movie, actually, did, have, on...  \n",
            "\n",
            "[50000 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "#text tokenizing\n",
        "imdb_data['processed'] = imdb_data['text'].apply(lambda x: nltk.word_tokenize(x))\n",
        "print(imdb_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_GH_ZVMeoT2",
        "outputId": "2289d637-6d41-4a0e-9594-536a7491d91a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      id                                               text label  rating   \n",
            "0   4715  For a movie that gets no respect there sure ar...   pos       9  \\\n",
            "1  12390  Bizarre horror movie filled with famous faces ...   pos       8   \n",
            "2   8329  A solid, if unremarkable film. Matthau, as Ein...   pos       7   \n",
            "3   9063  It's a strange feeling to sit alone in a theat...   pos       8   \n",
            "4   3092  You probably all already know this by now, but...   pos      10   \n",
            "\n",
            "                                           processed  \n",
            "0  movie gets respect sure lot memorable quotes l...  \n",
            "1  bizarre horror movie filled famous faces stole...  \n",
            "2  solid unremarkable film matthau einstein wonde...  \n",
            "3  strange feeling sit alone theater occupied par...  \n",
            "4  probably already know additional episodes neve...             id                                               text label  rating   \n",
            "49995  11513  With actors like Depardieu and Richard it is r...   neg       1  \\\n",
            "49996   5409  If you like to get a couple of fleeting glimps...   neg       1   \n",
            "49997  11187  When something can be anything you want it to ...   neg       1   \n",
            "49998   9359  I had heard good things about \"States of Grace...   neg       3   \n",
            "49999  11556  Well, this movie actually did have one redeemi...   neg       1   \n",
            "\n",
            "                                               processed  \n",
            "49995  actors like depardieu richard really hard task...  \n",
            "49996  like get couple fleeting glimpses cleavage att...  \n",
            "49997  something anything want mean bound register so...  \n",
            "49998  heard good things states grace came open mind ...  \n",
            "49999  well movie actually one redeeming quality made...  \n"
          ]
        }
      ],
      "source": [
        "#stop word, white space, special character removal, contraction expansion, lowercase transformation\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "new_stopwords = [\"would\",\"shall\",\"could\",\"might\", 'br']\n",
        "stop_words.extend(new_stopwords)\n",
        "stop_words.remove(\"not\")\n",
        "stop_words = set(stop_words)\n",
        "\n",
        "def remove_special_char(text):\n",
        "    clean_text = re.sub(r'[^a-zA-Z\\s]','', text)\n",
        "    return clean_text\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    clean_data = []\n",
        "    for i in text.split():\n",
        "        if i.strip().lower() not in stop_words and i.strip().lower().isalpha():\n",
        "            clean_data.append(i.strip().lower())\n",
        "    return \" \".join(clean_data)\n",
        "\n",
        "def expand_contractions(text):\n",
        "    contractions_dict = {\n",
        "    \"ain't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"would've\": \"would have\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"it's\": \"it is\" }\n",
        "\n",
        "    #regular expression pattern to find contractions\n",
        "    contractions_pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b')\n",
        "\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0)]\n",
        "\n",
        "    #Use regular expression to find and replace contractions\n",
        "    expanded_text = contractions_pattern.sub(replace, text)\n",
        "\n",
        "    return expanded_text\n",
        "\n",
        "\n",
        "def data_cleaning(text):\n",
        "    text = expand_contractions(text)\n",
        "    text = remove_special_char(text)\n",
        "    text = remove_stopwords(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "imdb_data['processed'] = imdb_data['processed'].apply(lambda x: ' '.join(x))\n",
        "imdb_data['processed'] = imdb_data['processed'].apply(lambda x: data_cleaning(x))\n",
        "print(imdb_data.head(), imdb_data.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tds-0IyLvjHt"
      },
      "source": [
        "## Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZeiv9t0eoT4",
        "outputId": "95218d3a-3ec8-4574-b993-01b37c4e2d1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17967    not long jeff jarrett left wwf good spoke nigh...\n",
            "32391    loved movie since saw theater wil wheaton favo...\n",
            "9341     compromised fairly charming film liked art dir...\n",
            "7929     ralph bakshi films appear like twoedged swords...\n",
            "46544    roger corman non epic sundry bunch characters ...\n",
            "                               ...                        \n",
            "21243    another fine effort america underrated filmmak...\n",
            "45891    word honor erased vocabularies nations aggrava...\n",
            "42613    found movie complete waste minutes jones weird...\n",
            "43567    must rate worst films ever seen nt funny wife ...\n",
            "2732     not film entertaining excellent comedic acting...\n",
            "Name: processed, Length: 35000, dtype: object\n"
          ]
        }
      ],
      "source": [
        "#train-test set split\n",
        "X_train, X_test, y_train, y_test = train_test_split(imdb_data['processed'], imdb_data['label'], test_size=0.3, random_state=0, shuffle=True)\n",
        "print(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfhBZhY5ZrxD"
      },
      "source": [
        "## Bi-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Fw7eM8xPeoT4"
      },
      "outputs": [],
      "source": [
        "#Count Vectorizer with Bi-gram\n",
        "Vect = CountVectorizer(min_df= 5, ngram_range = (2,2))\n",
        "X_train_vector = Vect.fit_transform(X_train)\n",
        "X_test_vector = Vect.transform(X_test)\n",
        "\n",
        "feature_names = Vect.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzsXjpgFeoT4",
        "outputId": "88b85462-089f-4c50-88fb-9ea9321dd04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aamir khan' 'alien resurrection' 'anyone remember' 'bad poor'\n",
            " 'billy halop' 'captain america' 'child prodigy' 'conveyed film'\n",
            " 'definitely made' 'dubbed sound' 'enough worth' 'exactly wrong'\n",
            " 'feel worse' 'film within' 'four major' 'getting worse' 'got minutes'\n",
            " 'hell got' 'imdb says' 'keep show' 'least entertaining' 'like without'\n",
            " 'love must' 'many american' 'modern hollywood' 'movie tells'\n",
            " 'needed something' 'not series' 'old tradition' 'park not'\n",
            " 'plane crashes' 'probably way' 'really explored' 'role see'\n",
            " 'screaming like' 'sequences involving' 'sing song' 'star although'\n",
            " 'style filming' 'theater version' 'time rent' 'tv late' 'violence rape'\n",
            " 'well animated' 'work truly']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(35000, 89528)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(Vect.get_feature_names_out()[::2000])\n",
        "len(Vect.get_feature_names_out())\n",
        "X_train_vector.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzYP0iCuzZSJ"
      },
      "source": [
        "## SelectKBest for Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9Gs4zgT75A5",
        "outputId": "44ef3cf4-c500-44bf-e938-b922cba9635f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aamir khan', 'angles keeping', 'believe better', 'cast great', 'cool got', 'downright awful', 'even true', 'film dvd', 'full bad', 'gratuitous nudity', 'hurts film', 'lack character', 'location work', 'material director', 'movie sounds', 'not intelligent', 'one winner', 'plot points', 'really seeing', 'scifi movie', 'silly nt', 'story new', 'thinking get', 'type horror', 'well chosen']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(35000, 50000)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_selector = SelectKBest(chi2, k=50000)\n",
        "\n",
        "X_train_vect = feature_selector.fit_transform(X_train_vector, y_train)\n",
        "X_test_vect = feature_selector.transform(X_test_vector)\n",
        "\n",
        "selected_feature_indices = feature_selector.get_support(indices=True)\n",
        "selected_features = [feature_names[i] for i in selected_feature_indices]\n",
        "print(selected_features[::2000])\n",
        "X_train_vect.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgUz1qdAziqy"
      },
      "source": [
        "## Bi-gram feature importance with Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbcWSm4neoT4",
        "outputId": "26315d64-0e3b-4afc-d271-4fb31a779a40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.8859\n",
            "Fold 2: Accuracy = 0.8939\n",
            "Fold 3: Accuracy = 0.8927\n",
            "Fold 4: Accuracy = 0.8820\n",
            "Fold 5: Accuracy = 0.8817\n",
            "Variance of accuracy scores: 0.0000\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.87      0.85      0.86      7485\n",
            "         pos       0.86      0.87      0.86      7515\n",
            "\n",
            "    accuracy                           0.86     15000\n",
            "   macro avg       0.86      0.86      0.86     15000\n",
            "weighted avg       0.86      0.86      0.86     15000\n",
            "\n",
            "Accuracy: 0.8619333333333333\n"
          ]
        }
      ],
      "source": [
        "# Logistic regression for Bi-gram\n",
        "lgr = LogisticRegression()\n",
        "lgr.fit(X_train_vect,y_train)\n",
        "\n",
        "# cross-validation on training data\n",
        "scores = cross_val_score(lgr, X_train_vect, y_train, cv=5)\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
        "\n",
        "# Print the variance of the scores\n",
        "variance = scores.var()\n",
        "print(f\"Variance of accuracy scores: {variance:.4f}\")\n",
        "\n",
        "y_pred = lgr.predict(X_test_vect)\n",
        "\n",
        "accuracy = lgr.score(X_test_vect, y_test)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification report:\\n\", report)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyCwZ2Y7Pv3c"
      },
      "source": [
        "## Bi-gram feature importance on Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRI9mcRzPqFj",
        "outputId": "064c5f75-df8c-4063-9660-165a8249f317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.9099\n",
            "Fold 2: Accuracy = 0.9197\n",
            "Fold 3: Accuracy = 0.9181\n",
            "Fold 4: Accuracy = 0.9104\n",
            "Fold 5: Accuracy = 0.9163\n",
            "Variance of accuracy scores: 0.0000\n",
            "Accuracy: 0.8697333333333334\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.87      0.87      0.87      7485\n",
            "         pos       0.87      0.87      0.87      7515\n",
            "\n",
            "    accuracy                           0.87     15000\n",
            "   macro avg       0.87      0.87      0.87     15000\n",
            "weighted avg       0.87      0.87      0.87     15000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vect, y_train)\n",
        "\n",
        "#cross-validation on training data\n",
        "scores = cross_val_score(nb_classifier, X_train_vect, y_train, cv=5)\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
        "\n",
        "# Print the variance of the scores\n",
        "variance = scores.var()\n",
        "print(f\"Variance of accuracy scores: {variance:.4f}\")\n",
        "\n",
        "y_pred = nb_classifier.predict(X_test_vect)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN4z8ecDQVfH"
      },
      "source": [
        "## Bi-gram feature importance with Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsQr63MIQKaY",
        "outputId": "48438458-bad3-4dfc-a619-fca63f52253e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.7887\n",
            "Fold 2: Accuracy = 0.8054\n",
            "Fold 3: Accuracy = 0.8053\n",
            "Fold 4: Accuracy = 0.8066\n",
            "Fold 5: Accuracy = 0.7956\n",
            "Variance of accuracy scores: 0.0000\n",
            "Accuracy: 0.8008666666666666\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.78      0.83      0.81      7485\n",
            "         pos       0.82      0.77      0.79      7515\n",
            "\n",
            "    accuracy                           0.80     15000\n",
            "   macro avg       0.80      0.80      0.80     15000\n",
            "weighted avg       0.80      0.80      0.80     15000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(X_train_vect, y_train)\n",
        "\n",
        "#cross-validation on training data\n",
        "scores = cross_val_score(rfc, X_train_vect, y_train, cv=5)\n",
        "\n",
        "# Print accuracy for each fold\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
        "\n",
        "# Print variance of the scores\n",
        "variance = scores.var()\n",
        "print(f\"Variance of accuracy scores: {variance:.4f}\")\n",
        "\n",
        "y_pred = rfc.predict(X_test_vect)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfEUMgObN5mb"
      },
      "source": [
        "## Bi-gram feature importance on Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSm5l4XqRSwg",
        "outputId": "92395e59-05d3-4597-ad8a-1a44028dd042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.7229\n",
            "Fold 2: Accuracy = 0.7264\n",
            "Fold 3: Accuracy = 0.7290\n",
            "Fold 4: Accuracy = 0.7390\n",
            "Fold 5: Accuracy = 0.7237\n",
            "Variance of accuracy scores: 0.0000\n",
            "Accuracy: 0.7410666666666667\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.73      0.76      0.74      7485\n",
            "         pos       0.75      0.72      0.74      7515\n",
            "\n",
            "    accuracy                           0.74     15000\n",
            "   macro avg       0.74      0.74      0.74     15000\n",
            "weighted avg       0.74      0.74      0.74     15000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "decision_tree = DecisionTreeClassifier()\n",
        "decision_tree.fit(X_train_vect, y_train)\n",
        "\n",
        "#cross-validation on training data\n",
        "scores = cross_val_score(decision_tree, X_train_vect, y_train, cv=5)\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
        "\n",
        "# Print the variance of the scores\n",
        "variance = scores.var()\n",
        "print(f\"Variance of accuracy scores: {variance:.4f}\")\n",
        "\n",
        "y_pred = decision_tree.predict(X_test_vect)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXTn6WXmZeSm"
      },
      "source": [
        "## Uni-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eXX1d0eQeoT5"
      },
      "outputs": [],
      "source": [
        "Vect = CountVectorizer(min_df= 5, ngram_range = (1,1))\n",
        "X_train_vect_uni = Vect.fit_transform(X_train)\n",
        "X_test_vect_uni = Vect.transform(X_test)\n",
        "\n",
        "feature_names = Vect.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9X7LmR1Gmxm",
        "outputId": "efccaa1f-3bbb-497c-aeaf-967aefb7d53c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aa' 'afterwards' 'angel' 'ashamed' 'baking' 'benito' 'bobs' 'brokeback'\n",
            " 'capes' 'charitable' 'cliffhanger' 'complicates' 'cope' 'cruelties'\n",
            " 'decent' 'develops' 'distract' 'ds' 'emanuelle' 'ethel' 'fahrenheit'\n",
            " 'finn' 'founders' 'gavin' 'gotcha' 'haim' 'helicopters' 'horny'\n",
            " 'impatience' 'insecure' 'jacko' 'kazan' 'large' 'linnea' 'macintosh'\n",
            " 'materialistic' 'mikes' 'moron' 'nearing' 'nuts' 'ossessione' 'parrish'\n",
            " 'phases' 'poole' 'proceeds' 'questionable' 'recount' 'repulsive' 'roared'\n",
            " 'samourai' 'secretly' 'sheriff' 'skinned' 'sommer' 'stakes' 'stubbornly'\n",
            " 'swedes' 'tepper' 'tonic' 'try' 'unforgettably' 'veered' 'wardens'\n",
            " 'willowy' 'yugoslavia']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "32123"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(Vect.get_feature_names_out()[::500])\n",
        "len(Vect.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5jIrcDU31Zt"
      },
      "source": [
        "## SelectKBest for Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5IFEDGc30sm",
        "outputId": "a0204ade-284d-4f53-8d0f-19f4a99b9964"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aag', 'baffling', 'camera', 'considering', 'different', 'excuses', 'genre', 'hope', 'kelly', 'manny', 'needless', 'pinjar', 'remake', 'setup', 'sterling', 'tossed', 'wang']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(35000, 5000)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_selector = SelectKBest(chi2, k=5000)\n",
        "\n",
        "X_train_vect = feature_selector.fit_transform(X_train_vect_uni, y_train)\n",
        "X_test_vect = feature_selector.transform(X_test_vect_uni)\n",
        "\n",
        "selected_feature_indices = feature_selector.get_support(indices=True)\n",
        "selected_features = [feature_names[i] for i in selected_feature_indices]\n",
        "print(selected_features[::300])\n",
        "X_train_vect.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4UMs-8oZbje"
      },
      "source": [
        "## Uni-gram feature importance on Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJMVxQaceoT5",
        "outputId": "be9577c8-afb9-45bd-ff7a-649bb15c99a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.8830\n",
            "Fold 2: Accuracy = 0.8869\n",
            "Fold 3: Accuracy = 0.8907\n",
            "Fold 4: Accuracy = 0.8857\n",
            "Fold 5: Accuracy = 0.8854\n",
            "Variance of accuracy scores: 0.0000\n",
            "Classification report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.89      0.88      0.88      7485\n",
            "         pos       0.88      0.89      0.88      7515\n",
            "\n",
            "    accuracy                           0.88     15000\n",
            "   macro avg       0.88      0.88      0.88     15000\n",
            "weighted avg       0.88      0.88      0.88     15000\n",
            "\n",
            "Accuracy: 0.884\n"
          ]
        }
      ],
      "source": [
        "# Logistic regression for uni-gram\n",
        "lgr = LogisticRegression()\n",
        "lgr.fit(X_train_vect,y_train)\n",
        "\n",
        "# cross-validation on training data\n",
        "scores = cross_val_score(lgr, X_train_vect, y_train, cv=5)\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
        "\n",
        "# Print the variance of the scores\n",
        "variance = scores.var()\n",
        "print(f\"Variance of accuracy scores: {variance:.4f}\")\n",
        "\n",
        "y_pred = lgr.predict(X_test_vect)\n",
        "\n",
        "accuracy = lgr.score(X_test_vect, y_test)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification report:\\n\", report)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hnz-UxM7SAHy"
      },
      "source": [
        "## Uni-gram feature importance on Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwYZ0AfBR_hN",
        "outputId": "3dad3c77-6086-44f7-d6cc-8b25c73e08ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.8673\n",
            "Fold 2: Accuracy = 0.8667\n",
            "Fold 3: Accuracy = 0.8701\n",
            "Fold 4: Accuracy = 0.8630\n",
            "Fold 5: Accuracy = 0.8633\n",
            "Variance of accuracy scores: 0.0000\n",
            "Accuracy: 0.8504666666666667\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.84      0.86      0.85      7485\n",
            "         pos       0.86      0.84      0.85      7515\n",
            "\n",
            "    accuracy                           0.85     15000\n",
            "   macro avg       0.85      0.85      0.85     15000\n",
            "weighted avg       0.85      0.85      0.85     15000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vect,y_train)\n",
        "\n",
        "# cross-validation on training data\n",
        "scores = cross_val_score(nb_classifier, X_train_vect, y_train, cv=5)\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
        "\n",
        "# Print the variance of the scores\n",
        "variance = scores.var()\n",
        "print(f\"Variance of accuracy scores: {variance:.4f}\")\n",
        "\n",
        "y_pred = nb_classifier.predict(X_test_vect)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzmJu6YvS-Nv"
      },
      "source": [
        "## Uni-gram feature importance with Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cui8rgANSnif",
        "outputId": "c101413b-fd59-4f9c-b4a0-7c351bb1cc01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.8537\n",
            "Fold 2: Accuracy = 0.8476\n",
            "Fold 3: Accuracy = 0.8499\n",
            "Fold 4: Accuracy = 0.8441\n",
            "Fold 5: Accuracy = 0.8543\n",
            "Variance of accuracy scores: 0.0000\n",
            "Accuracy: 0.8478\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.84      0.85      0.85      7485\n",
            "         pos       0.85      0.84      0.85      7515\n",
            "\n",
            "    accuracy                           0.85     15000\n",
            "   macro avg       0.85      0.85      0.85     15000\n",
            "weighted avg       0.85      0.85      0.85     15000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(X_train_vect,y_train)\n",
        "\n",
        "# cross-validation on training data\n",
        "scores = cross_val_score(rfc, X_train_vect, y_train, cv=5)\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
        "\n",
        "# Print the variance of the scores\n",
        "variance = scores.var()\n",
        "print(f\"Variance of accuracy scores: {variance:.4f}\")\n",
        "\n",
        "y_pred = rfc.predict(X_test_vect)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhCfjO2UUGZq"
      },
      "source": [
        "## Uni-gram feature importance with Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbKqQcZGTjuF",
        "outputId": "042eee4f-92f4-49bc-fc94-a8535924b12d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.7300\n",
            "Fold 2: Accuracy = 0.7276\n",
            "Fold 3: Accuracy = 0.7414\n",
            "Fold 4: Accuracy = 0.7260\n",
            "Fold 5: Accuracy = 0.7269\n",
            "Variance of accuracy scores: 0.0000\n",
            "Accuracy: 0.7287333333333333\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.81      0.59      0.69      7485\n",
            "         pos       0.68      0.86      0.76      7515\n",
            "\n",
            "    accuracy                           0.73     15000\n",
            "   macro avg       0.75      0.73      0.72     15000\n",
            "weighted avg       0.75      0.73      0.72     15000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "d_tree = DecisionTreeClassifier(max_depth=10)\n",
        "d_tree.fit(X_train_vect,y_train)\n",
        "\n",
        "# cross-validation on training data\n",
        "scores = cross_val_score(d_tree, X_train_vect, y_train, cv=5)\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
        "\n",
        "# Print the variance of the scores\n",
        "variance = scores.var()\n",
        "print(f\"Variance of accuracy scores: {variance:.4f}\")\n",
        "\n",
        "y_pred = d_tree.predict(X_test_vect)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnDl2rQxaMtA"
      },
      "source": [
        "## Uni and Bi-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "J1cpygvU_4ZA"
      },
      "outputs": [],
      "source": [
        "#Count Vectorizer with Uni and Bi-gram\n",
        "Vect = CountVectorizer(min_df= 5, ngram_range = (1,2))\n",
        "X_train_vect_mixed = Vect.fit_transform(X_train)\n",
        "X_test_vect_mixed= Vect.transform(X_test)\n",
        "\n",
        "feature_names = Vect.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAm2TCDwAfbv",
        "outputId": "a33d6e95-e713-4e3f-d71c-af74bc6a8932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aa' 'alone woods' 'around someone' 'become instant' 'borne' 'cemented'\n",
            " 'coldest' 'credited director' 'dialogue bit' 'early one' 'etc always'\n",
            " 'fake looking' 'film rented' 'four kids' 'give enough' 'grotesque'\n",
            " 'hollywood actor' 'instantly forgettable' 'killed man' 'licks'\n",
            " 'looking around' 'man real' 'mirth' 'movies let' 'night hunter' 'objects'\n",
            " 'oversee' 'pioneers' 'probes' 'really man' 'role comes' 'screen yet'\n",
            " 'shah rukh' 'solar system' 'still us' 'synthesizer' 'thought watch'\n",
            " 'truly magnificent' 'vanish' 'weak even' 'wormhole']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "121651"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(Vect.get_feature_names_out()[::3000])\n",
        "len(Vect.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwgyLdG9POdf"
      },
      "source": [
        "# SelectKBest for Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcgOxaRpPNwR",
        "outputId": "07f550bf-9ec0-49db-8e36-f2034bbda1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['aaa', 'better role', 'cover girl', 'ever since', 'gimmickry', 'james mcavoy', 'making character', 'not hollywood', 'predictable movie', 'see money', 'supposed budget', 'violent storm']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(35000, 70000)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feature_selector = SelectKBest(chi2, k=70000)\n",
        "\n",
        "X_train_vect = feature_selector.fit_transform(X_train_vect_mixed, y_train)\n",
        "X_test_vect = feature_selector.transform(X_test_vect_mixed)\n",
        "\n",
        "selected_feature_indices = feature_selector.get_support(indices=True)\n",
        "selected_features = [feature_names[i] for i in selected_feature_indices]\n",
        "print(selected_features[::6000])\n",
        "X_train_vect.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWrcQjl8ABvP"
      },
      "source": [
        "## Uni and Bi-gram feature importance with Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhjVr-_6AA8D",
        "outputId": "06bf996e-5203-4528-9034-86e2370d7849"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jacklynjoaquin/miniconda3/envs/minimal_ds/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.9033\n",
            "Fold 2: Accuracy = 0.9033\n",
            "Fold 3: Accuracy = 0.9031\n",
            "Fold 4: Accuracy = 0.8994\n",
            "Fold 5: Accuracy = 0.8980\n",
            "Variance of accuracy scores: 0.0000\n",
            "Accuracy: 0.8980666666666667\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.90      0.89      0.90      7485\n",
            "         pos       0.89      0.90      0.90      7515\n",
            "\n",
            "    accuracy                           0.90     15000\n",
            "   macro avg       0.90      0.90      0.90     15000\n",
            "weighted avg       0.90      0.90      0.90     15000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lgr = LogisticRegression()\n",
        "lgr.fit(X_train_vect,y_train)\n",
        "\n",
        "# cross-validation on training data\n",
        "scores = cross_val_score(lgr, X_train_vect, y_train, cv=5)\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
        "\n",
        "# Print the variance of the scores\n",
        "variance = scores.var()\n",
        "print(f\"Variance of accuracy scores: {variance:.4f}\")\n",
        "\n",
        "y_pred = lgr.predict(X_test_vect)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj9lWA4_AqQS"
      },
      "source": [
        "## Uni and Bi-gram feature importance with Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY4YdS0FAquO",
        "outputId": "091b5e2e-5baa-4211-8d40-8ec00ff6c163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.9004\n",
            "Fold 2: Accuracy = 0.9047\n",
            "Fold 3: Accuracy = 0.9067\n",
            "Fold 4: Accuracy = 0.8981\n",
            "Fold 5: Accuracy = 0.9009\n",
            "Variance of accuracy scores: 0.0000\n",
            "Accuracy: 0.8748666666666667\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.87      0.88      0.87      7485\n",
            "         pos       0.88      0.87      0.87      7515\n",
            "\n",
            "    accuracy                           0.87     15000\n",
            "   macro avg       0.87      0.87      0.87     15000\n",
            "weighted avg       0.87      0.87      0.87     15000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "M_NB = MultinomialNB()\n",
        "M_NB.fit(X_train_vect,y_train)\n",
        "\n",
        "# cross-validation on training data\n",
        "scores = cross_val_score(M_NB, X_train_vect, y_train, cv=5)\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
        "\n",
        "# Print the variance of the scores\n",
        "variance = scores.var()\n",
        "print(f\"Variance of accuracy scores: {variance:.4f}\")\n",
        "\n",
        "y_pred = M_NB.predict(X_test_vect)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZeO57AlAz44"
      },
      "source": [
        "## Uni and Bi-gram feature importance with Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO4963eFA5Eh",
        "outputId": "6e94d7cd-032e-4500-b4e0-16fcda6ecb0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.8641\n",
            "Fold 2: Accuracy = 0.8574\n",
            "Fold 3: Accuracy = 0.8653\n",
            "Fold 4: Accuracy = 0.8604\n",
            "Fold 5: Accuracy = 0.8590\n",
            "Variance of accuracy scores: 0.0000\n",
            "Accuracy: 0.8618\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.86      0.86      0.86      7485\n",
            "         pos       0.86      0.86      0.86      7515\n",
            "\n",
            "    accuracy                           0.86     15000\n",
            "   macro avg       0.86      0.86      0.86     15000\n",
            "weighted avg       0.86      0.86      0.86     15000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "R_F = RandomForestClassifier()\n",
        "R_F.fit(X_train_vect,y_train)\n",
        "\n",
        "# cross-validation on training data\n",
        "scores = cross_val_score(R_F, X_train_vect, y_train, cv=5)\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
        "\n",
        "# Print the variance of the scores\n",
        "variance = scores.var()\n",
        "print(f\"Variance of accuracy scores: {variance:.4f}\")\n",
        "\n",
        "y_pred = R_F.predict(X_test_vect)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfbQMOkHA5jw"
      },
      "source": [
        "## Uni and Bi-gram feature importance with Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bTVD8MYBHHj",
        "outputId": "133c880a-bf0a-4a06-8cd9-3f0ad4f98d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1: Accuracy = 0.7314\n",
            "Fold 2: Accuracy = 0.7263\n",
            "Fold 3: Accuracy = 0.7443\n",
            "Fold 4: Accuracy = 0.7254\n",
            "Fold 5: Accuracy = 0.7304\n",
            "Variance of accuracy scores: 0.0000\n",
            "Accuracy: 0.7322\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.82      0.60      0.69      7485\n",
            "         pos       0.68      0.87      0.76      7515\n",
            "\n",
            "    accuracy                           0.73     15000\n",
            "   macro avg       0.75      0.73      0.73     15000\n",
            "weighted avg       0.75      0.73      0.73     15000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "d_tree = DecisionTreeClassifier(max_depth=10)\n",
        "d_tree.fit(X_train_vect,y_train)\n",
        "\n",
        "# cross-validation on training data\n",
        "scores = cross_val_score(d_tree, X_train_vect, y_train, cv=5)\n",
        "\n",
        "# Print the accuracy for each fold\n",
        "for i, score in enumerate(scores):\n",
        "    print(f\"Fold {i+1}: Accuracy = {score:.4f}\")\n",
        "\n",
        "# Print the variance of the scores\n",
        "variance = scores.var()\n",
        "print(f\"Variance of accuracy scores: {variance:.4f}\")\n",
        "\n",
        "y_pred = d_tree.predict(X_test_vect)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\\n\", report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "minimal_ds",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
